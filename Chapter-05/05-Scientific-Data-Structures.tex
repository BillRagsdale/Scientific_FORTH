\chapter{Scientific Data Structures}
\startcontents[chapters]
\printcontents[chapters]{}{1}{}

\TallC{D}{ata} structures are the soul of any computer program in any language. Some languages, most notably FORTRAN and BASIC, predefine some data structures but require extensive contortions to define others. This straitjacket approach has virtues as well as defects:

\begin{itemize}
    \item The re-defined structures are what most users need to solve stan ard problems, so meet 80-90\% of the cases in practice. That 15, they are not terribly restrictive.
    \item Because the most-needed structures are predefined and have a standard format, they do not have to be invented each time a prggram is written. Standardization facilitates the exchange an portability of programs.
    \item Standardized data structures aid program development in discrete modules, permitting sections written by different persons or teams to interface properly with minimal tuning.
\end{itemize}

FORTH pre—defines a minimal set of data structures but allows unlimited definition of new structures. How is this different from Pascal, Ada, or even C? FORTH not only permits extension of the set of data structures, it permits definition of new operators on them. Thus, \eg, FORTH permits simple implementation of complex arithmetic whereas the aforementioned do not.

This chapter\footnote{Much of the material in this chapter has appeared previously in J.V.Noble, \textit{J.FORTH Ap. and Res.} 6 (1990) 47.} suggests protocols for arrays and typed data\footnote{Most languages classify data by type: in FORTRAN, \eg, we have INTEGER, INTEGER*4, REAL, REAL*8, COMPLEX, and COMPLEX*16, requiring 2, 4, 4, 8, 8, and 16 bytes of memory, respectively.} that will increase the portability of code and encourage the exchange of scientific programs. The keys to this are generic operations that recognize the data type of a scalar or array variable at run—time and act appropriately.

\section{Typed data structures}

One of the virtues of FORTRAN or BASlC is that the programmer does not have to keep track of what type of data he is fetching and storing from memory. In fact the user does not even program such operations explicitly - the compiler takes care of everything induding the bookkeeping. Mixed-arithmetic expressions like

\begin{lstlisting}
Z = -37.2E-17*CEXP(CMPLX((R**2,W)/32)/DSIN(W)
\end{lstlisting}

place great demands on a compiler. The compiler first tabulates the types of the variables and literals in the expression, and then decide which run-time routines to insert. With two types of integers and four types of floating-point numbers (REAL*4, REAL*8, COMPLEX*8, and COMPLEX*16) a typical binary operator such as exponentiation (** offers 36 possibilities. No wonder FORTRAN compilers are slow. 

\TallC{F}{ORTH} sacrifices automation opting for a small, fast, flexible compiler. The traditional FORTH style gives each type of data its own operators. However, if a program demands all the standard REAL*4, REAL*S, COMPLEX*8 and COMPLEX*16 data types (not to mention INTEGER*2 and *4), having to remember them all and use them appropriately is a chore. This problem has

led me to experiment with generic access operators, \bc{G@} and \bc{G!}. These let FORTH keep track of which words to use in fetching and storing the "scientific" data types to the fstack (which may partly reside on a co-processor like the 80x87 or MC68881 chips). Corresponding generic unary and binary floating point operators \bc{GDUP}, \bc{G*}, \textit{etc}. allow programs themselves to be generic.

\TallC{I}{} have lately further modified the scheme to permit more complete automation. The kernel of the method is an "intelligent" fstack, or ifstack, that recordns the type of each number on it. The generic arithmetic operators and library functions decide from the information on the ifstack how to treac their operands.

An ifstack-base protocol for floating point and complex arithmetic has drawbacks and advantages. A major drawback is the run-time overhead in maintaining the ifstack and in choosing the appropriate operator for a given situation. In other words we trade convenience for a non-negligible execution speed penalty. To some extent this can be mitigated by \textit{computing} decisions and by vectoring rather than branching (\textit{i.e.} no Eaker \bc{CASE} statements or \bc{IF ... ELSE ... THEN}s). Moreover, although the definitions are coded in high-level FORTH for portability, the key words should be hand-assembled for the target machine. Finally, my high-level ifstack manager has plenty of error checking that could be dispensed ith when speed is an issue.

The chief advantages of the ifstack are:
\begin{itemize}
    \item Unlike FORTRAN, this scheme permits generic routines that will accept several types of input. Hence, \eg, a matrix inversion routine will happily invert REAL*4, REAL*8, COMPLEX, and DCOMPLEX matrices.
    \item A FORTRAN \rightarrow FORTH translator\footnote{See J.V. Noble, \textit{J. FORTH Ap. and Res.} 6 (1990) 131. See also Chapter 11, where we describe a FORmula TRANslator} becomes simple with generic operators.
    \item The ifstack permits recursive programming \textit{a la} LISP.
\end{itemize}

\subsection{Type descriptors}
\TallC{T}{o} decide at run-time which \regc{@} or \regc{!} to use for a particular datum, FORTH needs to know what type of datum it is. The scheme described here wastes a little memory by attaching to each variable a label that tells \bc{G@} and \bc{G!} how to get hold of it.

Here is how we label types:
\begin{lstlisting}
    \ Data type identifiers
    0 CONSTANT REALM \ 4 bytes long
    1 CONSTANT REAL‘8 \ 8 bytes long
    2 CONSTANT COMPLEX \ 8 bytes long
    3 CONSTANT DCOMPLEX \ 16 bytes long

    \ a simple version of #BYTES
    CREATE #bytes 4 C, 8 C, 8 C, 16 C,
    : #BYTES (type — #bytes) #bytes + C@ ;
\end{lstlisting}

\subsection{Typed scalars}

\TallC{W}{e} want the machine to remember for us the data-specific fetches and stores to the co-processor. To accomplish this, the typed variable has to place its address and type on the stack. Thus we need a data structure that we might visualize diagramatically in Fig. 5-1 below (a cell  represents 2 bytes):

Fig. 5-1 Memory structure a! a typed scalar

We implement a scalar through the defining word

\begin{lstlisting}
    : SCALAR ( type--)
       CREATE DUP , #BYTES ALLOT
       DOES> DUP@ SWAP 2+ SWAP ; ( -- adr t )
\end{lstlisting}

The word \bc{SCALAR} is used as
\begin{lstlisting}
REAL*4   SCALAR X
REAL*8   SCALAR XX
COMPLEX  SCALAR Z
DCOMPLEX SCALAR ZZ
  etc.
\end{lstlisting}

\subsection{Defining several scalars at once}
\TallC{O}{ne} aspect of the FORTH method of handling variables, that seems strange to programmers familiar with Pascal, BASIC, or FORTRAN, is that \bc{VARIABLE}, \bc{CONSTANT} or a new defining word like \bc{SCALAR} need to be repeated for each one defined, as above. That is, such defining words generally do not accept name-lists.

This idiosyncracy can be traced to FORTH’s abhorrence of variables:
\begin{itemize}
    \item Easily read (and maintained) FORTH code consists of short definitions with few (generally $\leq$ 4) numbers on the stack. Such programs have small use for variables, especially since the top of the return stack can serve as a local variable.
    \item In FORTH as in BASIC, variables tend to be global and hence corruptible. The variables in a large program can have unmnemonic names or names that do not express their meaning simply because we run out of names.
    \item Experienced FORTH programmers tend to reserve named variables for such special purposes as vectoring execution.
    \item The standard FORTH kernel therefore discourages named variables by making them as tedious as possible.
\end{itemize}

Most objections to variables can be resolved by making them local. Local variables are relatively easy to define in FORTH: a straightforward but cumbersome method for making "headerless" words is given in Kelly’s and Spies’s book\footnote{M.G. Kelly and N. Spies, \textit{Forth, a Text and Reference} (Prentice-Hall, New Jersey, 1986), p. 324 ff.}.

HS/FORTH\footnote{Harvard Softworkts, PO. Box 69, Springboro, Ohio 45066 Tel: (513) 748-0390.} provides beheading in a particularly simple form: \bc{BEHEAD' NAME}, or \bc{BEHEAD" NAME1 NAME2}.

Used after \bc{NAME} has been invoked in the words that need to reference it, \bc{BEHEAD'} removes \bc{NAME}’s dictionary entry leaving pointers and code fields intact and recovering the unused dictionary space. The more powerful word \bc{BEHEAD"} does the same for the range of dictionary entries \bc{NAME1 ... NAME2}, inclusive.

Beheading variable or constant names makes them local to the definitions that use them; they cannot be further accessed -or corrupted- by later definitions. (Pountain\footnote{Dick Fountain, "Object-oriented FORTH", Byte Magazine, 8/86; \textit{Object—oriented FORTH} (Academic Press, Inc., Orlando, 1987).} has given yet another method for making variables local, using a syntax derived from "object-oriented" languages such as SMALLTALK.)

Variables are essential for scientific programming. Since we must often have more than two variables, it is silly to repeat \bc{SCALAR}. A simple way to allow SCALAR to use a list is

\begin{lstlisting}
    :SCALARS ( n -- )
      SWAP 0 DO DUP SCALAR LOOP DROP ;
    \ Examples:
    \ 2 REAL*4  SCALARS  A  B
    \ 5 COMPLEX SCALARS XA XB XC XD XE
\end{lstlisting}

I find the use of \bc{SCALARS} with modifiers and lists more convenient and readable than many repetitions of \bc{SCALAR}. Its resemblance to FORTRAN (thereby helping me live with my FORTRAN-inspired habits) is pure coincidence. Although possible to use a terminator (",\eg) rather than a count (to define the variable list) I feel it is desirable for the programmer to know how many variable names he has supplied, hence the counted version.

\subsection{Generic access}
\TallC{A}{} major theme of FORTH is to replace decisions by calculation henever possible\footnote{Leo Brodie, \textit{Thinkning FORTH} (Prentice-Hall, Inc., Englewood Cliff, NJ, 1984), p. 118ff. See also J.V. Noble, "Avoid Decisions", \textit{Computers in Physics} 5,4 (1991) 386.}. This philosophy usually pays dividends in execution speed and brevity of code.

But there is an even more important reason to avoid \bc{IF ... THEN} decisions, especially when working with modern microprocessors. CPUs like 80x86 and MC680x0 achieve their speed in part by pre-fetching instructions and storing them in a queue in high speed on-chip cache memory. A conditional-branch machine instruction (the crux of \bc{IF ... THEN}) empties the queue whenever the branch is taken. Branches should be avoided because they slow execution far more than one might expect based on their clock-counts alone.

To replace decisions, we use the standard FORTH technique of the execution array (analogous to the familiar assembly language jump table). This lets us compute from the type descriptor which fetch or store to use.

\TallC{W}{e} now define \bc{G@} and \bc{G!} as execution arrays using\footnote{HS/PORTH uses a word pair \bc{CASE: ... ;CASE} that performs the same task as \bc{G: .. ;} below. \bc{G:} was inspired by Michael Ham (\textit{Dr. Dobb's Journal}, October 1986).} anexecution-array-defining word \bc{G:}

\begin{lstlisting}
    : G: CREATE ]
       DOES> OVER + + @ EXECUTE ; ( t -- )
    G: G@ R32@ R64@ X@ DX@ ;
    G: G! R32! R64! X! DX! ;
\end{lstlisting}

assembled from components of the FORTH compiler. That is, the ordinary colon \bc{:} might have the high-level definition (shorn of error detection)

\begin{lstlisting}
    : : CREATE ] DOES> @ EXECUTE ;
\end{lstlisting}

\bc{CREATE} makes the new dictionary entry, and \bc{]} switches to
compile mode. \bc{DOES>} specifies the run-time action (recall any word created by \bc{CREATE} leaves its parameter field address
\textbf{-pfa-} on the stack at run-time, \textit{prior} to the actions following \bc{DOES>}). In the case of \bc{:} the run-time action is to fetch the pfa of the new word and execute it. At run-time, words defined using \bc{G:} add twice the type descriptor to the pfa (to get the offset into the array) then fetch the desired address and \bc{EXECUTE} it.

Microprocessors like the MC680x0 and 80386 that can address large, level memories require no further elaboration for \bc{G@} and \bc{G!}. However, if large arrays are to be addressed within the segmented memory addressing protocol of the 8086/80286 chips, we would have to define \bc{G@} and \bc{G!} to use the “far” forms of addressing words\footnote{Consult, \eg, LJ. Scanlon, \textit{op. cit.}; or R. Lafore, \textit{op. cit.} HS/FORTH defines "far" access operators, \bc{@L} and \bc{IL} of all types, that expect a "long" address on the stack. For example, \bc{CODE R32®L DS POP. FWAIT. DS:[BX] DWORD-PTR. FLD. END-CODE}}. For example, in HS/FORTH such words as \bc{R32@L} expect a segment paragraph number and offset (32 bits total) as the complete address of the variable being fetched to the 87stack. In that case we modify the definition of \bc{SCALAR} to include the segment paragraph number (seg) in the definition (\bc{LISTS} is nonstande - it is HS/FORTH’s name for the portion of the dictionary containing the word headers)

\begin{lstlisting}
    : SCALAR (type - -)
    CREATE DUP , \make header ,type
    #BYTES ALLOT \reserve space
    DOES > > R
    [ LISTS @] LITERAL (- - sag)
    R@ 2 + (- - seg off)
    R> @ - (--segofltype)
    \ Ex: BEAU-4' SCALAR x
\end{lstlisting}

\subsection{The intelligent floating point stack (ifstack)}
\TallC{T}{he} \textbf{ifstach} is a more complex data structure than either a simple fstack or the parameter/retum stacks. When a typed datum is placed on the ifstack its type must be placed there also.

But the typed data have varying lengths, from 4 to 16 bytes. We
can deal with this two different ways: either ALLOT enough
memory to hold a stack of the longest type, making each position
on the ifstack 18 bytes wide (to hold datum plus type); or manage
the ifstack as a modified heap, with the address of a given datum
being computable from the ifstack-pointer and the data type.

The 18-byte wide ifstack wastes memory, but is easy to program.
(In retrospect, this is exactly the method I used to program
adaptive numerical quadrature10 .) After several false attempts I
settled on the fixed-width ifstack. High level FORTH code for
this variant is given below.

\TYPEDDATASTACKMANAGER (-—segoll‘type)

TASKFS’TACKS \aay. REALM SCALAR X

fiND CP@ 0= :SCALARS (ntype--)

?(fi.OAD(X)MPLEX.fiH) SWAP 0 DO DUP SCALAR LOOP
DROP ;

\ddinedata—typetokens \say4DOOMPLEXSCALARSXAXBXCXD

0 CONSTANT REALM

ICONSI’ANT REAL'B \definsionstortheparalelstackottypesanddm

2 CONSTANT COMPLEX \Brodle.TF(Brady,NY.1984) p. 207.

3 (DNSTANT DCOMPLEX

CREATE FSTACK 2018' 2+ ALLOT

CREATEitbyteu 0.80.86. 16 C. \2tos-pouer2018-bytecels

:#BYTES #bytes + C@ ; :FSJNIT FSTACK 0!:

(M38"thlflesl =>EMPTY ("8°90“)

[USTS@]UTERAL

\ddlnesodarandscalars FSTACK DUP@ 18' 2+ +;

:SCALAR (type--) :>FS (segotltype--) \myzx >FS
CREATE DUP, #BYTESALLDT >R >EMPTY (uwgdfseg'oif)
DOES> DUP@ SWAP 2+ SWAP ; R@ OVER l \storetypeonlstack

 

 

10. LV. Noble, “Scientific Computation in FORTH”. Computers in Physics 3 (1989) 31; and
GI- mus of this book.

OWVMIM—Mmm.

100

Chaplets - Sclentltlc Data Stntcturea

Scientific FORTH

FS> (segciltype--)\aay:X FS> @EXECUTE : (t--)
FSTACK 1-l \decllstackptr
>R >EMPTY(-- segoiiseg'oit') G: G@ R32@L R64@L X@L DX@L ;
DUP@ R@ = \srce.type=dea.type? G: Gl R321L R641L XlL DXlL ;
IF 2+ DSWAP (-- seg'oif'segoil) \movedatatromliatacktoflromFPU
R> #BYTES (--seg'oi'l"segoiln) :FS>F (--t 87:--x)
CMOVEL \movedatairomltstack FSTACK 1—l \declatackptr
ELSE RDROP CR >EMPTY (--eegoi‘l)
ATTEMPT TO STORETO DUP@ >R 2+
WRONG DATATYPE" ABORT R@ (-- segoiftype)
THEN G@ R>:
\movedatal'romltstacktosntackJeavetype
\ execution-anay defining word
\HS/FORTHhasthefaster :F>Fs (t-- 87:x--)
\CASE:...;CASEpairtorthesarne]ob >R >EMPTY (-- segoil)
R@ OVER!
:6: CREATE ] DOES> OVER+ + 2+ R> (--segcirtype)

 

The stack comments and comments should make the preceding
code self-explanatory.

§§6 Unary and binary generic operators

We want to define generic unary and binary operators whose

run-time action selects the desired operation using informa-
tion contained in the ifstack. A unary operator such as FNEGATE
or FEXP expects one argument and leaves one result. With a
floating-point coprocessor (FPU) the only distinction is between
real or complex. This distinction is contained in the second bit of
the type descriptor, which we exhibit in Table 5-1 on page 101,
in binary notation.

Real and complex can then be distinguished via the code fragment

2 AND (type - - O = wall 2 = complex)

ms-MMW 101

Thu. 5-1 Bit-patterns of data type descriptors

 

Type BINARY representation
REAL-4 00000000 00000000
REAL'B 00000000 00000001

COMPLEX 00000000 00000010

DCOMPLEX 00000000 00000011

 

 

 

Since most unary operators produce results of the same type as
their argument, we write a defining word for generic unary oper-
ators:

: GU: CREATE ] DOES> (--pfa)

FS> F (- - pfa t) \ get data
UNDER 2 AND + (- - t adr)
@ EXECUTE \ do it

F > FS ; \ return ans.

When we use GU: in the form
GU: GNEGATE FNEGATE XNEGATE ;

CREATE produces a dictionary entry for GNEGATE; ] turns on
the compiler so the previously defined words FNEGATE and
XNEGATE have their addresses compiled into GNEGATE’s
parameter field; and DOES > attaches the run-time code. The
run-time code converts the real/complex bit into an offset, 0 or 2
which is added to the address of the daughter word to get the
address where the pointer to the actual code is stored. This
pointer is fetched and EXECUTEd.

A few unary operators like XABS (complex absolute value)
return real values from complex arguments. If we want to use
GU: to define, say, GABS, we must remember to redefine
XABS so it zeros the second bit of the type descriptor left on

OJulanVNobleteez—Alrlghtsraserved.

102

Chapter 5 — Sciemlllc Data Stnrctures Scientific FORTH

the stack, before returning its result to the ifstack. This is just a ‘I
AND so is fast.

A binary operator (one that takes two arguments) expects its
arguments and their types on the ifstack. There is no distinction
between single- and double-precision arithmetic on most
numeric coprocessors. However, the result must leave the proper
type-label on the stack. Here is what we want to happen, illus-
trated in fig. 5-2 as a matrix TYPE(arga, argb)

 

 

TYPE“,

 

Dx__x ox x ox

 

 

 

flg. 5-2 Types resulting from 2—argument operators

Note: this protocol avoids misleading precision for the results
of computations. It seems more scientific than FORTRAN’s
“convert intermediate results to the precision of the highest-
precision operand” protocol.

 

If we think of the indices and entries in fig. 5-2 as numbers 0, 1,
2, 3 (so we can use them as indices into a table) rather than as let-
ters, a simple algorithm emerges: the first bit of the result is the
logical-AND of the first bits of the two operands, and the second
bit of the result is the logical-OR of their second bits. Although
we would program this in assembler for speed, the high-level
definition is

MLMMW 103

:NEW.TYPE ab--a2+b2+a1b1)
--abab)
--abab)
1AND --ab [ab])1
—ROT OR in [ab1a+b)
2AND (--[ab1a+b]2)
+ ; (--a2+b +a1b1)

Since only logical operations are used. NEWJYPE is faster than
table lookup or branching. Note that in programming this key
word we have obeyed the central FORTH precept: “Keep it
simple!" by choosing a data structure (the numeric type tokens
0-3) that is easily manipulated.

We will also need a way to select the appropriate operator from
a jump table of addresses. Given that the precision (internal) is
irrelevant, again all that matters is whether the number is real or
complex, Le. the second bits of the numbers. The first operation
must then be to divide by 2 (right-shift by one bit). We then have
the matrix of fig. 5-3 below

 

 

0 1 0 1
RR RX 0
XR XX 1

 

 

 

flg. 54! Operatoraelectlon matrix

where R stands for real-real, etc. The numerical elements are
generated as 2'] +1. This leads to the word

:WHICH.0P (ab--c)
2/ SWAP 2 AND + ;

104

Chapter 5 - Scientific Data Structures Scientific FORTH

Thus we come to the binary generic-operator defining word

:68: CREATE ] DOES> ( -ap1‘ )
FS>F FS>F (- -pfa t0 t1)
NEW.TYPE UNDER (--t’p1at')
WHICH.OP 2* + \ make result-type
@ EXECUTE \ select binop
F > FS ; \ save result
\say: GB: 6* F* F‘X X‘F X‘ ;

The generic multiply G', e.g., picks out, at run-time, which of
four routines to use. By using only logical or shift operations we
have made even the high-level definitions fairly quick in com-
parison with the times of floating point operations.

The only instance where one might forego the overhead penalty
paid for the convenience of generic coding would be in nested
inner loops, such as occur in matrix operations. Here it might pay
to code four inner loops, one for each type, and then access them
generically, e.g.

:RLOOP real words ;
: DRLOOP dreal words ,
:XLOOP complex words ,
:DXLOOP dcomplex words ;

G. GLOOP RLOOP DRLOOP XLOOP DXLOOP;

§2 Arrays of typed data

Numerical arrays represent a frequently encountered charac-

teristic feature of scientific programming. Arrays per se are
hardly foreign to FORTH. Arrays of typed data are novel, how-
ever, and therefore worth elaborating. Following Brodie’s ad-
vice(TF, p. 48ft") we first specify the “user interface” (matrix
notation) and then proceed to implementation.

mmm ms-mmm 105

“I W (FORTRAN-lite) array not-Ion
Something like V(15) —the 15'th element of V— is the com-
monest notation for array elements in high-level languages
because lineprinters and terminals do not easily recognize sub-
scripts. In FORTH, the most natural notation would be postfix
(RPN), 15 v —but this is both hard to read and unintuitive“.
That is, 15 V does not say, immediately and unambiguously,
“I am the 15th element of the array V l"

FORI'H's idiosyncrasies forbid saying V(15) because the parser
recognizes V(15) as a single word . Since we want the 15 to be
parsed, we would have to modify the FORTRAN-ish notation to
V0 (0 15) or V(. 15 -), where °stands for a blank space (ASCII 32).
Unfortunately, “(” is a reserved word. While we might place the
matrix definitions in a separate vocabulary —which would let us
redefine anything we want— “(" is too useful as a comment
delineator to dispense with.

This leaves the second possibility, where “(” becomes part of
the array name, V(. To make V(- 15 -) work,“ ) ” must become
an operator —unless we want to leave postfix notation entirely,
with all the complication that would entail”. Since “ ) " is not a
reserved word, nothing in principle prevents defining it as an
operator. However, such usage would conflict with comments.

The square braces, [ ], are commonly used in matrix notation;
however both are reserved FORTH words, Le. forbidden. This
leaves the curly braces { }, which are unused by FORTH.

Ofthe two possible forms, V- {0 15 } or V{° 15° }, the latter has
the advantage that the opening brace, {, is only part of the name,
but reminds us that the name V{ is an array, exactly as names
ending with 3 are strings, etc. The notation suggests a further

 

1. Other authors have noted this and proposed more readable matrix notations. See, e.g., Joe
Bentham, “FORTH and the Fast Fourier Transform" Dr. Dobb‘r Journal, September, 1984, p.
34. Also Did: Pountains's book Object-oriented FORTH (op cit.) uses an array naming conven-
tion with brackets.

RecaflthatthestandardFORTHdelimiteristheASCllblank(20h - 32d).

See. e.g., L Brodie, WFORW (op. cit.) p. 1135.

Pt“

eJuanvuouoim—Mmm.

106 Chapter 5 - Sclentlflc Data Structures Sclentlflc FORTH

mnemonic refinement, namely to place {{ and }} at the ends of
2-dimensional arrays, as in M{{' 3 ° 5 ° }}.

How will this notation operate? Clearly, to place the (general.
ized) address of the n’th element (of a l-dimensional array) 01
the stack we would say

W“ n '}.

whereas

M{{~ m- n -}} i

should analogously place the address of the m,n’th element of a
2- dimensional array on the stack. 1

§§2 Large matrices
The defining word SCALAR given in §2 above allots space 111
the dictionary —for most FORTHs, code + data must fit
here— or in the LISTS segment of HS/FORTH (part of the
dictionary). This is OK for variables, but not for arrays, since even
a modest matrix would exhaust the ( $64 Kbyte) LISTS segmem.

A REAL“ matrix uses 4 bytes per ellement. The largest such arrq
that can be stored in a 65, 536 (i. e., 21) -byte segment is 128x 1233
This" Is the largest array that can be addressed with unsigned 16- '
numbers. On the other hand, a filled IBM PC/XT clone has 64
Kbytes of memory under MS—DOS. Even a generous F0
kernel (plus DOS) takes up less than 150 K; hence 450 K '
available to hold large arrays. Up to 8 Mbytes can be added
EMS storage, assuming a suitable memory managem
scheme 14. That is, in principle one could tackle matrix proble
of order 350X350. What about speed? The dominant term '
solving linear equations by —say- Gaussian elimination '
partial pivoting is

 
   
 
 
  
   
 
  
 
 
  

 

14. See, e.g., Ray Duncan, “FORTH support for Intel/Lotus expanded memory”, Dr. Dobb's I
August 1986; also, John A. Lefor and Karen Lund, “Reaching into expanded memorf', PC 1'
Journal, May 1987.

awe-WWW 107

13
T§mn

where m is the time for 1 multiply and 1 add, and n is the order
of the matrix. We should also include the fetch + store time, since
the bus bandwidth is as much a limiting factor as the FPU arith-
metic speed. For the 8086/8087 the time m is of order 400 clock
cycles. Thus the asymptotic execution time on a 10 MHz machine
should be of order 10 minutes for n = 350.

On the 80386/80387 combination running at 25 MHz, the execu-
tion time for the same problem should be only 2.3 minutes or so.
Thus it would be practical (Le, execution time I! 1 hour) on such
machines, even without special equipment such as the [IT 80c387,
or an array co-processor, or a faster proced re such as Strassen’s
algorithm (see Ch. 4 §8), to tackle 103 x 103 dense matrix
problems.

The crucial question therefore, is memory. The Intel machines
were designed around a segmented memory architecture. That is,
to avoid having to use (expensive) 32-bit address registers, the
8086/80286 chips were designed to use 16-bit registers. However,
these chips have more than 16 external address lines — 20 for the
8086, 2A for the 80286. Thus the absolute address is compounded
of two numbers: a segment descrlptor and an offset which
must be present in appropriate registers. The segment descriptor
is the absolute address of a l6—byte paragra h, divided by 16.
The offset is any (unsigned) integer from 0 to 2 6— 1 = 65,535 that
can fit in a 16-bit register.

The chips contain 4 segment registers: SS (stack segment), CS
(code segment), DS (data segment) and ES (extra segment). five
registers, BP, SP, SI, DI and BX, can be used for offsets, although
they are not entirely interchangeable (some have specific func-
tions in some of the more complex machine instructions, such a
string operations). Manifestly, since the 8086 can address

22° =1,048,576 bytes ('1 megabyte”),
the largest segment number is 214_1 = 16,383.

A typical (segmented) address is expressed in Intel assembly code
as

emvmtm-umm.

108 Chapter 5 - Scientific Data Structures Sclem'flc FORTH

cs: [BX+S|+0008]

which translates in words to “add the offset in BX to that in SI
and then add 8 to get the total offset; take the segment descriptor
in CS, multiply by 16 and add to produce the absolute address.

§§3 Using high memory

HS/FORTH permits accessing all the memory in a PC/AT (up
to 1 megabyte) in the following manner:

0 Define a named segment of length 1 byte: this marks the be-
ginning of available memory.

0 Then tell both FORTH and DOS how much memory you want.

 

As might be expected, HS/FORT'H defines non-standard worth r
(coded as DOS function calls) to use the various DOS service
routines that allocate memory, etc.

MEMORY 4+ @ S->D
DCONSTANT MEM.START \beg. of free memory
40.960 DCONSTANT MAX.PARS
\40960 = 655360 /16
:TOTAL.PARS MAX.PARS MEM.START D- ;
\ # pars of memory available 3:
1 SEGMENT SUPERSEG }
\ define named segment 1 byte long ="
TOTALPARS DROP FREE-SIZE
\ tell DOS and HS/FORTH about it

   
 
    
  

Having allocated the memory, how can we address it efficien .
We would like the simplicity of double-length integer arithme
for computing an (absolute) array address, as in

abs.adr(A,-,) = abs.adr(AOO) +(row.length"l+J) *#B

However, although the absolute address referenced by a segm
and offset is unique, i.e. the absolute address in bytes is

 

15. Sec, e.g., D.N. Jump, Programmer’s guide to Ills-DOS, rev. ed. (Brady Books, New York, 1987).

abaadr -18'segment + offset,

the reverse translation, of an absolute address (in bytes) to the
segment + offset notation expected by 801186 processors is not
unique. This naturally poses a problem when the processor tries
to prevent segments from overlapping (protected mode). In such
cases, the only answer is a memory management scheme that
computes segments and offsets (by brute force) in a nonooverlap-
ping fashion. For example, we might define large arrays such that
each row has its own segment paragraph.

The 80386 CPU has a third mode that permits direct 32-bit
addressing of 4 gigabytes (albeit few computer users have quite
this much fast memory available). A scheme for addressing large
amounts of RAM in 80386 machines (without leaving MS-DOS)
has been discussed in Dr Dobb’s Journal“.

' For 8086 PC's and/or real-mode programming on 80286+

machines, we can merely ignore whether segments overlap.
Oddly, standard assembly programming booksl omit this way of
addressing segmented memory.

The 8086 permits 32— bit addressing as long as we translate 32-bit
addresses to the segment + offset notation expected by the 80:86
processors in real mode. A word that performs this conversion is
SEG.OFF, defined as

: >SEG.OFF (d - - seg off)
OVER 15 AND (- - d off)
-ROT D16] DROP (- - off seg)
SWAP ;

The 32-bit address is placed on the stack as a double-length
integer, with the low-order (tie. offset part) above the segment
part. The phrase OVER 15 AND saves bits 0-3 (of the 32-bit

 

5. See,e.g.,AlWil|ia1ns, “DOS + 386 = 4Gigabytes", Dr. Dobb'slournal, July 19%, p. 62.

7.

C. Morph and M. Waite, was/m l6-bil War primer (Byte/McGraw—Hill, Peter-
borough, 1982); L SmhmlBMPCéXTassanblymmgudeforW
(Bfldy/Preadce-Hall. Bowie, Md, 1983); a. [afanssembly m primer form lBMPCd
XT (Plume/Waite, New York, 1984).

emvmrm—me.

110 Chapters—SciemlfchataStmctwu SclenfiflcFOR

address); 410T 0/16 DROP then shifts the (32-bit) addres
right 4 bits and drops the least-significant part, to produce
offset. This conversion method produces offsets in the ran
00-0F (hex), that clearly have nothing to do with the original offs:
(that led to the 32-bit absolute address via 16‘seg + off ).

§§4 A general typed-array deflnltion .
or the new syntax to work the word } must compute the address:
of the n’th element of V{ from the information on the staclg,

and }} must do the same for M{ {. In order to encompass:
matrices of typed data we specify that the results of the phrases}
V{ n } and M{{ m n }} be to leave the generalized address our
the parameter stack, 1'. e. to leave the stack picture ( - - seg off typefi,
exactly as with SCALARS.

Before we can define }, however, we must specify the data:
structure it operates on, i.e. the array header.

Once again we begin with the user interface. We can opt fol.
maximum generality or maximum simplicity. My first attempt fel.‘
into the first category, permitting the user to define a named
segment of given length and to define an array in that segment
Lately I realized this generality accomplishes little, so have abut?
doned it. All arrays will be defined in the heap, named SUPER;-
SEG as above. To define a length- 50 1ARRAY of 4-byte numbea
we will say

50LONG REALM 1ARRAY V{

Now, before we work out the mechanics of 1ARRAY, we imagim
that an array will be stored as in fig. 5-4 below:

The proposed data structure consists of an 8-byte header (the
array descriptor) in the dictionary (LISTS in PIS/FORTH), wifi
the body of the array stored elsewhere. The array descriptfi;

it

points to the absolute address of the array data (body). 1,

rmmm Mabmmm 111

 

" LISTSponion:(V{putsadron

 

, wens + mascara
M” ”‘9‘" (324a address)

1 l

lfi%l WWI ”8% l%%l —~ ”WWhUW-‘r

.

. SUPERSEG portion to Wuunemory h SUPERSEG
lfifillfiSt llfillfi fill%%ll%%ll%5’l —-

L—d start at data - vases (from beg. or SUPERSEGU

fig. 54 Structure of a 1-dimenslonal array In SUPERSEG

 

 

 

 

 

 

 

 

 

 

 

 

 

The array-defining word 1ARRAY must perform the following
tasks:

a lace the length and type of the data in the first two cells
4 bytes) of the array descriptor.

a lace the 32-bit address (start of data) in the next two cells
4 bytes) of the array descriptor.

a allot the necessary storage in SUPERSEG.

e at run-time, place the generalized address, length and type on
the parameter stack.

The start of data is handled by VHERE, a word that puts the next

vacant (32-bit) address in SUPERSEG on TOS.

We define VALLOT to keep track of the storage used by arrays.
VALLOT increments the pointer in VHERE (and aborts with a
: warning if the segment length is exceeded).

We first define some auxiliary words:
MEMORY 4 + @
S- > D DCONSTANT MEMSTART
\ bag. of free memory

emvmrm-Mmm.

112

Chapter 5 — Sclentllc Data Structures Scientific FORTH

40.960 DCONSTANT MAXPARS \4oeeo= 655360/10

:TOTAL.PARS MAX.PARS MEM.START D-
\ # pars avail. mom.

1 SEGMENT SUPERSEG \ named seg. 1 byte long
TOTALPARS DROP

FREE-SIZE \tell DOS and HS/FORTH

DVARIABLE VHERE>
:lNlIVHERE> 0.0 VHERE> Dl

a
l

lNlIVHERE >
: VHERE ( - - d.offset) VHERE) D@ ;
: D4/ 02/ 02/ ; : 016/ D4/ 04/ i

: TOO-BIG? VHERE >SEG.OFF
0 AND TOTALPARS D >
ABORT“ INSUFfiCIENT ROOM IN SUPERSEG' ;
\ check whether new value of VHERE > passes end
\ of SUPERSEG

: VALLOT (d.#bytes - -)
VHERE D + DDUP TOO.BIG?
VHERE > D! ;

Array-defining words
Ex: 50LONG REAL*4 1ARRAY V{

17} >FS (::--V[17])

: LONG DUP ;
fiNDD, 0= ?( :D, SWAP, , ;)
\ conditionally compile D,
:1ARRAY (IIt--)
CREATE UNDER D, \t,l into 1st4bytes(--|t)
SUPERSEG @ 16 M* \ start of SUPERSEG

VHERE D + D, \ abs. address —~next 4 byt
#BYTES M“ ( I t - - #bytes to allot)
VALLOT ; \ allot space in the segmen

\ run-time action: (- - adr)

comment Ml-MMW 113

We also need some words to go with 1ARRAY:

(adr n - - seg. oti[n] t)
SWAP DUP@ R > \fypo .. rstack
4 + D@
ROT R@ #BYTES M'
D+ >SEG.OFF R > ; (- - seg.cfl[n] t)

finally, here is a useful diagnostic word

\begin{lstlisting}
    :7TYPE (t--) \it's ok for this to be slow!
        DUP 0 = IF DROP ." REAL*4"   EXIT THEN
        DUP 1 = IF DROP ." REAL*8"   EXIT THEN
        DUP 2 = IF DROP ." COMPLEX"  EXIT THEN
        DUP 3 = IF DROP ." DCOMPLEX" EXIT THEN
        ." NOT A DEfiNED DATA TYPE" ABORT ;
\end{lstlisting}

\subsection{2ARRAY and \}\} }
\TallC{W}{e} now want to define arrays of higher dimensionality. For example, to define a 2-dimensional array we might say

\begin{lstlisting}
    90 LONG BY 90 WIDE COMPLEX 2ARRAY XA{{
\end{lstlisting}

This leads to the definitions
\begine{lstlisting}
    : BY ; \a do-nothing word for style
    : WIDE * ; (I w -- I*w)
    : 2ARRAY   (I*w t —- ) 1ARRAY ;
\end{lstlisting}

Now let us define \}\} to fetch the double-indexed address:

\begin{lstlisting}
    : }} ( adr m n -- a[m*I+n] t)
        >R OVER 2+ @ ( -- adr m I*w)
        * R> + } ;
\end{lstlisting}

By correct factoring (putting some of the work into \bc{WIDE}) we achieved an easy definition of \bc{2ARRAY}. Careful factoring also let
us define \}\} in terms of \}.

\section{Tuning for speed}

Some of the words in our typed-data/matrix lexicons should be optimized or redefined in machine code. Accessing matrix elements imposes a non-trivial overhead on matrix operations. We can reduce the execution time with inline code, either in the traditional FORTH manner via selected assembler definitions, or with a recursive-descent optimizer such as HS/FORTH’s\footnote{J.S. Callahan, \textit{Proc. 1988 Rochester FORTH Conference} (Inst. for Applied FORTH Research, Inc., 1988), p. 39.}.

Experience teaches that optimization is most fruitful (most bang for the buck) applied to entire inner loops and other selected areas of code, rather than to access words \textit{per se}. By hand-coding the innermost loop in matrix inversion and FFT routines, one achieves programs that run in (asymptotically) minimum time on the 8086/8087 chip set.

\TallC{S}{ignificant} speed increases in data access could perhaps be obtained with multiple code field (MCF) words, as described by Shaw\footnote{G. Shaw, "Forth Shifts Gears, I", \texit{Computer Langmge} (May 1988) p. 67; "Forth Shifts Gears, II", \textit{Computer Language} (June 1988) p. 61; \textit{Proc. 9th Asilomar FORML Conference} (JFAR 5 (1988) 347.)}, and as implemented by HS/FORTH in the words \bc{VAR}, \bc{AT}, \bc{IS}, and variants thereof. The disadvantage of MCF style is that compile-time binding, while faster in execution, loses the flexibility of run-time binding. That is, data types would - as with FORTRAN - be specified at compile—time, and lexicons would be recompiled to run with specific types. Run-time binding as described in this Chapter produces \texit{generic} words that can handle all four standard scientific data types, a major advantage over MCF.

\leftbar[1\linewidth]
FORTH data structures, especially as defined in this Chapter, do little- or no bounds checking, hence do not prevent accidentally overwriting key parts of the operating system.

The new fetch and store words were defined in high-level FORTH for safety. Adding bounds-checking to arrays, at least during the debug cycle, is \underline{strongly recommended} to avoid crashing, or even damaging, the system.
\endleftbar