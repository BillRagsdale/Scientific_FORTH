\chapter{The 80x87 Family}
\startcontents[chapters]
\printcontents[chapters]{}{1}{}

\TallC{W}{hen} we speak of the 80x87 mathematical co-processor family, we include the original Intel chips, the Cyrix D387 and HT C287 and C387 chips, and the AMD 80287 and 80387 clones, as well as the on-chip floating point unit found on the Intel 80486 chips.

We now describe some features of the 80x87 floating point co-processors (FPUs) that affect scientific programming on IBM-PC compatible machines\footnote{Although we confine ourselves to the 80x87 chip, the Motorola 68881/2 coprocessors can be programmed in  the same general manner to achieve floating point capabilities rivialling VAX minicomputers.}. The 8087 chip complements the Intel 8088/8086 CPU family, the 80287 works with the 80286 series, the 80387 with the 80386, and the 80486 includes an on-chip FPU. The 8087 chip is connected pin-for-pin to the 8088/8086. (The details are given in \textit{The 8087 Primer\footnote{J. Palmer and S. Morse, \textit{The 8087 Primer} (John Wiley & Sons, NY (1984), hereafter referred to as \textbf{8087P}).} The interface and instruction set automatically take care of bus arbitration (that is, which chip has access to the memory) and interrupts, in order to be sure that the CPU and 80x87 do not perform conflicting operations.

Instructions for the 80x87 are always appended to a code called “escape” (ESC, D8h) that alerts the coprocessor and diverts control to it. The MS-DOS assembler MASM and debugger DEBUG will automatically assemble this code with 80x87 instructions, so the user does not need to worry about including ESC except to be aware that it is happening. (Of course, a FORTH system that lacks 80x87 assembler extensions will need to include ESC explicitly to generate 80x87 codes.)

We shall see in Ch. 4 §1.1 how to use the FORTH assembler, and in Ch. 4 §1.2 how to use DEBUG\footnote{A treasure included with MS-DOS}, to extend a FORTH system for 80x87 operations if they are not already included as a floating point lexicon.

The 80x87 machine code instruction set includes instructions for moving numbers to the registers from memory and vice-versa, as well as from one 80x87 register to another. The internal moves are of course much faster than those to or from external memory.

Advanced programming methods — such a recursive algorithms — require an fstack of unlimited depth. The designers of the 8087 anticipated the need for fstack extension and included instructions for this purpose. Unfortunately, the instructions were not well thought out\footnote{see \textbf{8087P}, p. 93ff.} so a moderately complex software fstack extension manager is needed to augment them. We design such a manager in Ch. 4 §7.

\section{Internal 80x87 and manipulation}

The 80x87 is organized around a stack of 8 80-bit registers (the87stack). The 8-deep stack can be subdivided into smaller stacks for special purposes, but this is only useful when coding in assembler for speed\footnote{see \textbf{8087P, p. 87ff.}.

We begin with words for performing 80x87 stack manipulation analogous to those defined for parameter stack. These are FDUP FSWAP FDROP FROT FOVER .

How are we to define them in terms of machine code primitives?

\subsection{The FORTH assembler}

very FORTH worthy of the name includes an assembler,

usually set up as an alternate vocabulary\footnote{Vocabularies are a method for subdividing the dictionary.} called ASSEMBLEH.
The assembler allows direct definition of a new word in terms of
machine codes, which are referred to using standard mnemonics. A
typical FORTH assembly language definition (we now specialize to
HS/FORTH) for @ would have the form\footnote{BX is a CPU register, and \[BX\] means "the memory location wose address is in BX". HS/FORTH uses a naming convention in which assembler mnemonics end with a period, \textit{e.g.} MOV, . Also, HS/FORTH makes the TOS the BX register, to reduce the number of pushes and pops needed to execute simple words.}

CODE @ BX [BX] MOV. END-CODE

ACODE definitionis a machine-coded subroutine somewhere
in memory. To use it, the compiler has to know where it is and
insert appropriate unconditional JUMP (JMP) and RETURN
(REI‘) instructions in the machine code representation of the
calling program.

Here is what happened in the CODE version of @ above:

0 The defining word CODE set up a dictionary entry with the
name @, with an appropriate pointer and JUMP instructions
to make the newly defined word run the code sequence com-
prising the definition.

 







Chapter 4 — The N? Famly Scientific FORTH

e The word END-CODE cleans up the loose ends by adding
the obligatory RET instruction sequence and turning off the
compiler. (That is, END-CODE installs “NEXT".)

0 END-CODE is thus the analog of ; as CODE is of 3 .

Consider now the word FROT whose Intel assembly language
definition would be

FROT: ; entry point

FWAIT ; hold 8086 operations
FXCH ST(1) ; swap TOS and NOS

FWAIT ; hold 8086 operations
FXCH ST(2) ; swap TOS and ST(2)
RET ; return

In PIS/FORTH assembler, the definition becomes 1 .
CODE FROT FWAIT. 1 FXCH. FWAIT. 2 FXCH. E
END-CODE 1

Note: the definition includes FWAIT (the same as WAlT), an 1‘
instruction that makes the 8086 CPU wait for the FPU to com- i
plete its work before attempting to access the memory. If WAlT ,
were omitted and the CPU accessed the memory, it could store ‘.

incomplete resultss. i

 

§§2 Using MS-DOS DEBUG ‘
The FORTH assembler, with its 80x87 extension, lets us
develop machine-coded words while retaining Intel mnemon»
ics for documentation, at the price of loading and compiling the:
entire ASSEMBLER lexicon. But if we know the actual machine .
code bytes we can bypass the assembler by entering the (hexae "
decimal) codes directly into a CODE definition. Most FORTH’

 

1a

The design of the 80286, 80386 and 80486 eliminates this problem. consequently WAIT is not
required when assembling 80287/80387/80487 machine code. See, ¢.g., John H. Crawford and
Patrick P. Gelsinger, Programming the 80386 (Sybex. San Francisco, 1987). HS/FORTH allows
the user to choose which class of machine to assemble for, when loading the 80:87 assembler c!-
lension. The 80287 + option simply defines FWAIT. as a null word. '

l1

 

mam Mo-msomF-uy 69

system, in addition to an assembler, provide a way to insert
machine codes —hex numbers - directly into the code field of a
word. HS/FORTH, e.g., uses the words < 96 and 96 > to enclose
the hex codes being inserted.

The problem is, how do we find out what these hex codes are?
The simplest way is to use DEBUG9 to generate (HEX) code

sequences. Rather than try to explain, we shall illustrate by re-
cording and annotating the DEBUG session for the word FROT:

C > DEBUG starts DEBUG
-A100 Assemble from man
3801:0100 FWAIT enter asembler
3801:0101 FXCH ST(1) mnemonlcs

3301:0103 FWAIT
3301:0104 FEXCH ST(2)

" Error DEBUG notes a typo
3801:0104 FXCH ST(2)
3801:0106 no more, < cr>stops assembly
-U 100 105 Unassemble to check
3801:0100 98 WAIT Hold CPU

3801:0101 0909 FXCH ST(1)
(87:abc -- acb)
3801:0103 98 WAIT Hold up CPU
3801:0104 DQCA FXCH ST(2)
(87:acb--bca)

-D 100 105 Qump to get hex codes
3801:0100 98 09 09 98 09 CA
-0 Quit session

From the Dump (or Unsassembly) we find code bytes 93 D9 F7
D9 C9 F6 D9 C9 which can be inserted directly into the defini-
tion of FROT:

 

9. see, eg, R. Lafitte, Assembly urging: Primer for the IBM PC & XT (Plume/Waite -New
American Library, New York, 1%). Complete operating systems often include a code debug-
get that permits My; disassembly; modifying the contents of selected memory locations; set-
ting breakpoints; and running proyams under debuger control.

emvuounese-Almm.

70

Chapter 4 — The ma? anly Scientific FORTH

CODE FROT <96 9809 F709 CSFB 0909 96>
END-CODE (::abc--bca)

The rest of the 80x87 stack words, l

FDUP FSWAP FDROP FOVER F-ROT ‘

whose assembler definitions are:

l
CODE FDUP FWAIT. 0 flD. END-CODE l
CODE FSWAP FWAIT. 1 FXCH. END-CODE
CODE FDROP FWAIT. o FSTP. END-CODE L
CODE F—ROT FWAIT. 2 FXCH. .
FWAIT. 1 FXCH. END-CODE i
l

 

can be defined similarly in 8086/8087 machine code using the
DEBUG program or a reference manual for the chip (cg. 8087?) 1
to determine the hex codes.

§2 Memory usage (storage and retrieval)

he 80x87 instruction set includes codes for loading ST (0)

from memory and storing ST(O) to memory. The former invole
ves a “push” and the latter may or may not involve a “pop", from
the 87stack.

For the moment we need words to retrieve and store 16-bit
integers, short reals (32 bit) and temporary reals (80 bit).
These have mnemonics fiLD (“load integer to ST (0)”), fiSTP
(“store integer and pop ST(1) into ST(O)”); flD, FSTP respec-
tively. Typical (HS/)FORTH assembler definitions are

CODE I16@
FWAIT. [BX] WORD-PTR fiLD.
[3x1 POP.
FWAIT.

END-CODE

CODE l16|
FWAIT. [BX] WORD-PTR fiSTP.
[13x1 POP.
FWAIT.

END-CODE

ma—mmrm 71

Similarly, m and I32! can be defined by replacing WORD-
PTR with DWORD-PTR. 11) define M and l6“ -assuming
these are needed— replace DWORD-PTR with OWOBD-PTR.
The 32-, 64- and 80-bit floating point analogues 332@, R32!,
Rm, RMI, ROO@ and ROOI are definedbyreplacing fiLD
by flD and fiSTP by FSTP, and using DWOflD-PTR,
OWORD-PTR or Terra-Pram as appropriate.

We also need words to load the 87stack from the stack and vice
versa. In HS/FORTH, the top of the parameter stack is
actually the BX register on the CPU. There is no machine instruc-
tion for loading the 87stack directly from a CPU register. Thus,
we must first transfer the contents of BX to memory and thence
to the 87stack. The inverse operation also must proceed through
a memory location. The data-transfer words are named in an
obvious way 5- > F and F- > S. HS/FORTH defines them directly
in machine code, manipulating the CPU register BP that points
to the top of the CPU stack. That is, HS/FORTH uses two bytes
immediately below NOS as the intermediate memory cell.

Here we define S— > F and F— > S directly in high-level FORTH
by wasting a little memory for a (hidden) temporary variable:

VARIABLE TEMP
:S—>F (n-- ::--float[n])
TEMP I \TOS —> TEMP
TEMP l16@ : \TEMP-> ST(O)
: F—>S (::x — --int[x])
TEMP I16! \ST(O) -> TEMP
TEMP @ ; \TEMP—> TOS

BEHEAD' TEMP \ hide address or TEMP

Faster machine code versions of S—> F and F— > S are11

CODE S—>F TEMP +[] BX MOV. BX POP.
FWAIT. TEMP +[] I16 fiLD. END-CODE

CODE F—>S 8X PUSH. TEMP +[] fiSTP.
ex TEMP +[] MOV. END-CODE

 

10. “Ten-byte pointer". Note PIS/FORTH appends a period “." to most Intel mnemonics.
ll. TEMP +[] isHS/FORTH’sphrase to assemble anamedmemoryaddress.

DJunnvuounm—umm.

72

Chapter 4 — The m7 Famly Sclentlflc FORTH

These definitions will satisfy our present needs for storing and
retrieving from the 87stack.

Note: a substantial gain in speed can be achieved with the
80386/80387 and 80486 families, by using instructions that effect
32-bit wide transfers 2.

\section{Arithmetic words}

\TallC{F}{PU} (80x87) arithmetic is generally performed with the
maximum precision allowed by the (80-bit) size of the
registers .

As noted in §4.2, the 80x87 allows 3 floating point representations
for storage and retrieval in external memory: 32 hit (single precision),
64 bit (double precision) and 80 bit (“temporary real”).

To conserve memory we generally use 32 bit floating point num-
bers (REAL'4 in the old FORTRAN parlance)unless the nature
of the calculation demands retention of more significant figures
to prevent roundoff errors.

The FORTH arithmetic words we shall need are

F+ F— FR— F* F/ FR/ FNEGATE FABS FSGN
whose definitions are (CODE and END-CODE are assumed)

F+ FWAIT. FADDP. (a7: a b -- a+b)
F- FWAIT. FSUBP. (87: a b -- a—b)
Ffi— FWAIT. FSUBRP. (87: a b -- b—a)
F* FWAIT. FMULP. (a7: a b -- a'b)
F/ FWAIT. FDIVP. (87: a b -- a/b)
Ffl/ FWAIT. FDIVRF’. (a7: a b -- b/a)

 

12.
13.

See, cg, John H. Crawford and Patrick P. Gelsinger,Progumming the 80386 (Sybex, San Fran-
cisco, 1987).
MthmrghhhptnsialewfmaudfidaflprredstfimnfisubismflmmaEarhhmedc
performedtmothermachinesohatis.toeompareresultswhiledebufing),lseenow'rtueinamode
thatdmnupcaktdmkmswhikdbrdnhldngpredsionandrefuthereadermnfs2a8

sammam own-name“ 73

FNEGATE FWAIT. FCHS. 87: e - - -e)
FABS FWAIT. FABS. 87: a -- lal)
:FSGN (n--87:x--|x 'sgn[n])

FABS 0< IF FNEGATE EN ;

54 Special constants

For convenience the designers of the 8087 chip have arranged

fast loading of certain constants into ST(O) of the fstack (1' OS).
The words that place these constants on the fstack, and the
corresponding assembler mnemonics and (hex) codes are shown
in Table 4-1 below.

 

Table 4-1 Speclal Constants

word oonst. mnemonic codes

F =0 0 flDZ 98 09 E5
F=1 1 flD1 93 De E8
FwPl pi=3.14... flDPI 98 09 E5
F=L2(10) Iog210 flDL2T 98 D9 E9
F =L2(E) logze flDL2E 98 09 EA
F =L10(2) logioZ flDLG2 98 09 EC
F=LN(2) loge2 flDLN2 98 09 ED

 

 

 

74 Chapter 4 — The com Femly Scientific FORTH

§5 Test words

We need to be able to determine the algebraic sign of a floating

point number, as well as whether one is larger than another.
The 80x87 chip has 4 instructions for this purpose, whose
mnemonics are FTST, FCOM, FCOMP and FCOMPP; they are
shown below in Table 4-2.

Table 4-2 Machine language floating point tests

 

mnemonic comparison pop?

Fr ST ST(O) to 0 no
FCOM ST(1) to ST(O) no
FCOMP ST(1) to ST(O) pop once
FCOMPP ST(1) to ST(O) pop twice

 

 

 

The results of these comparisons are encoded as bits (3 (14) and
C0 (8) of the 16-bit 80x87 STATUS register. In order to get these
bits by bit- masking techniques‘ the STATUS register must be
moved to the parameter stack1 .This 15 done with the the 80x87
instruction FSTSW via the assembler sequence

VARIABLE ESTATUS

CODE FSTSW 8X PUSH.
ESTATUS +[] FSTSW.
ex ESTATUS +[] MOV.

END-CODE

BEHEAD' ESTATUS

 

l4. mthe80387includesanewinstructionwherebythestatuseontrolmrdcanbemoveddirsct-
brintothcAXrey'sleroflhcmCPU. ThehucodesforFSTSWAXare DFED.

Now we have to consider how to bit-mask the status integer left
on the stack by FSTSW. We use the logical AND with 400011 og
010011 (Exam Why?) to pick out C3 and co. Now from usrr‘
we have the truth table 4-3 below:

M043 rmnmumrmmumxlsmopaw — Oor$T(l))

 

Conamon C3 co
srm)>x o 0
mm=x 1 o
mm<x o 1

 

 

 

Now we are in a position to define the test words F0>, F0=,
F0<,aswellasF>,F=,F<.Wehave16

CODE FTST FWAIT. FTST. 0 FSTP END-CODE
CODE FCOMPP FWAIT. FCOMPP. END-CODE

HEX
:FTSTP FTST FSTSW ;

: F0> FTSTP 4100 AND NOT 0);
I F0= FTSTP 4000 AND 0 > i

: F0< FTSTP 0100 AND 0> ;

: F< FCOMPP FSTSW 4100 AND NOT 0> ;
: F= FCOMPP FSTSW 4&0 AND 0>;

: F> FCOMPP FSTSW 0100 AND 0>§
DECIMAL

 

15. aee,e.g., Table 4.1

16. fluthetestwordsF< andF>aredefinedothetoFo> andFo<.Thisreversalofdirec-
tionsisnaratypographicalerrorfltisdemanded bytheoperationofFCONPP -seell7|‘.

thlthNoblHQZvAlrlgtlsrsaaNed.

76 Ctmpter4-Tham7Famly SclendflcFORTH

\section{Mathematical functions}

\TallC{W}{e} now proceed to develop a suite of special functions for the 801187 FPU. These will include the usual trigonometric functions, logarithms, and exponentials. We retain initial Fs' 1n the names to remind us the FPUis being used. The functions are given in Table 4-4 below:

Table 4-4 Mathematical function prlmltlvas

 

name action code(3) mnemonic
FSQRT (87: x - - t/i ) 98 09 FA FWAIT FSQRT
FY'LG2X (87: y x - - y*Iogz[x] ) 98 09 F1 FWAIT FYL2X

FY*LGZXP1 (37: y x - - y'logz[x+1] ) 93 09 F9 FWAIT FYL2XP1

F2XM1 (87:x-- 2"—1) 9809F0 FWAITFZXM1

 

 

 

These primitive functions allow us to define the logarithms and
exponentials. To get log2(x), for example, we need to decide
whetherx lies between 0 and 2: this can best be accomplished with
the sequence (we assume x is already on the 87stack)

: F=2 F=1 FDUP FSCALE FPLUCK ; (e7:--2)‘7

:LOG.TST FDUP FO> NOT
ABORT‘ Can't take Iog(—|x|) 11' ;

: flG (87: y x - - y*|g[x])
LOG. TST FDUP F- — 2
IF F= 1 F— FY'LGZXP1

ELSE FY'LGZX THEN ;

 

17. See below for a discussion of FSCALE.

ma-mwmrm 77

: flN F-LN(2 FSWAP flG ;
:flOG F-L10() FSWAP flG;

Now we can use the fundamental definition of exponentiation to define both the operation of raising an arbitrary positive number to a real power, as well as the standard mathematical function e’:

F2“ (87: x-- 2"x) F2XM1 F=1 F+ ;
F“ (87: xy-- y“x) flG F2“ ;
FEXP (87: x-- exp[x]) F=L2(E) F' F2“ ;

\TallC{W}{e} now have only to implement the trigonometric and inverse-
trigonometric functions. We need (TREAL) IO-byte constants:

: F, HERE 10 ALLOT R80| ;
: FCONSTANT CREATE F, DOES> REO@ ;

Also, for simplicity, we define FORTH functions for degree/radian conversions and conversely:

fiNIT F=Pl 130 s->r= fl (87:-- p/180)
FCONSTANT Pl/180 \ make constant
:DEG->RAD Pl/180 F* ;
:RAD->DEG PI/180 F/ ;

The 80x87 chip has a fast way to multiply or divide by powers of
2, called FSCALE. The CODE definition is

CODE FSCALE <96 98 D9 FC 96 > END-CODE

FSCALE adds ST(1) to the (powers-of-Z) exponent of ST(O).
Thus, cg. , we can write fast divide- and multiply-by-Z instructions:

: F2“ F=1 FSWAP FSCALE FPLUCK ;
: F2] F=1 FNEGATE FSWAP FSCALE FPLUCK ;

Now, according to 808‘", we may evaluate trigonometric and
inverse-trigonometric functions using the instructions

CODE FPTAN FWAIT. <96 DQFZ %> END-CODE
CODE FPATAN FWAIT. <96 D9F3 96> END-CODE

OWVMOHHOZ-Almm.

78

Chapter 4 - The m7 Fame Scientific FORTH

The 8087 and 80287 implement an unnormalized tangent func-
tion, whose effect is ( 87: z - - y x), with tan(z) =y/x. Let us define

f= tan(z /2) (1)

That is, we obtain the tangent of half the angle. The other trigon-

ometric functions can be computed in software using the iden-
tities

. = le/xl
srn(z) 1+ 0/102 (2)

= 1- gy/x22
cos(z) 1 + (”’02 (3)
mm = M (4)

A further problem created by the 8087/80287 instruction set is
that the argument 2 must lie in the range 0 < z < 11/4 . Thus we
must shift the argument to this range, using a special instruction
FPREM (“exact" partial remainder 8) that can be used to extract
multiples of :r :

CODE FPREM FWAIT. <96 DQ F8 96> END-CODE
: XDUP FOVER FOVER ;
: ENUF? FSTSW 1024 AND 0= ; \bit C2 =0?
: FNORM (87: x k - -x mod k) FSWAP
BEGIN FPflEM ENUF? UNTIL FPLUCK ;
\ extract multiples of R

Here is how we code the tangent in high-level FORTH:

WWI

x=0 is an exception - set tan =0 and exit.
x < 0 ? Save sign as a flag on stack.
reduce by multiples of n .

\ cont'd

 

18. wer', p. 1005

awn-moment» 79

x h1stquedrant(:r/2< x< n)? fieg,reduceby:r/2
x Intatoctantoru< x< 3/2)? flagroduoobynm

\ HIGH LEVEL FORTH VERSION
:REDUCE (:87:xk--xmodk --f)
XDUP F> DUP IF F- ELSE FDROP THEN ;

: FTAN (87:x--tan[x]) FDUP F0:
IF EXIT THEN \tan==0
FDUP FO< FABS (--Isgn 87:-- |x|)
F=PI FNORM \0< x< n
F=Pl F2/ REDUCE (--isgnf1q)
F=Pl F2] F2] REDUCE (--isgnf1q f1o)
FPTAN fl (87: |x|— -tan[x])
IF F=1 XDUP F+ F-ROT F- F/ THEN

\ adjust for octant

IF 1/F FNEGATE THEN \adjustfor quadrant
IF FNEGATE THEN ; \adjust sign

The remaining trigonometric functions (sine, cosine, scant, co-
secant) can easily be defined in terms of tan(z /2) . For example,
here are FSIN and FCOS:

:FSIN F2] FTAN FDUP FDUP F“ F=1 F+
FR/ F2* ;

: FCOS F21 FTAN FDUP F* F=1 FOVER F-
FSWAP F=1 F+ F/ -

\textbf{Note:} The 80387 improves on the 8087/80287 by eliminating the
need to adjust the argument in software. Further, the tangent
produced by the 80387 is normalized (that is, x = 1.0 in Eq. 4.1
above). finally, the 80387 has instructions FSIN, FCOS and
FSINCOS built in, so all the software emulation is unnecessary.

\TallC{W}{e} define inverse-trigonometric functions using FPATAN

defined previously, whose action is ( 87: y x - - arctan[y/x]).
The 80387 has no additional instructions for inverse trig func-
tions, relative to the 8087/80287, so the same code fits all.

To calculate the inverse functions, we make use of standard
identities (the forms chosen minimize roundoff error). Thus,

Arm“) = Armani (l-zz)(1+z)) (5)

eunumunaaz-umm.

n
Arccos(z)- 21mm“: +‘ } (6)
: FATAN F-1 FPATAN ;
:FASIN FDUP FABS F-i F>

ABORT' argu'nentotarcsh > 1'

F-1 FOVER FDUP F' F— Fsoa'r

F/ FPATAN ;

:FACOS FDUP FABS F-t F>
ABORT'uwmentotarccos>1'
FDUP F-1 FR- FSWAP F-1 F+ fl
FSORT FPATAN ;

m: the argument of arcsincr) or arccosbr) must be smaller than
1 in absolute value — hence we include a bounds check to avoid
taking the square root of a negative number.

 

\section{Extending the intrinsic 80x87 stack}
\TallC{A}{s} promised' in the beginning of the chapter, we now desip
afstackmanagerin softwarethatallowsmorethanScells. first -‘
we examine the structure of the 80:87 stack (87M).

From m we see that the 8 registers in the 87stack are:
organized as a circular stack. A 3-bit pointer, 51; records which:
physimlregisteris aemallyTOSJheinstruc-tionsetallowsSth
be incremented (fiNCSTP) or decremented (FDBCSTI’)?
modu108.1he87stackisshownbelowinfig.4-lonpage81.

A fiD instruction decrements ST (mod 8) before storing ,
whereasaFSTPimtruetionincrementsS’I'l‘obuildoursoftwart;

fstackweneedtodothefolknvingthinp:

e WhentheflstackgetshrleutSlKDonthememymmioe,
andvr'a-vasa.

eKeeptraekolhowmanymmbersareontheflstxk. f

o Keepuackofwherewehsvestoredthelastmmberremei
fromtheflstack.

-m-—A_.-U

Md-MWFUW 81

 

 

- E 4TH FROM TOP

5'TH FROM TOP

 

d

 

2 6'TH FROM TOP

 

 

 

3 7'TH FROM TOP

 

4 O'TH FROM TOP

 

5 1'ST FROM TOP

 

l6 2’ND FROM TOP

 

7 3'RD FROM TOP

 

 

 

 

 

fig. 4.1 The 00:87 snack. from seen:

The algorithm has the following expression in pseudo-FORTH

Redefine operations that put #5 on 87stack:
increment stack _pointer
it 87steck full put st(7) on fstack
push onto st(O)

Redefine operations that take #5 off 87stack:
popst(0) _
decrement stack _pornter
iffstack notempty. puttofs onto st(7)

We begin by defining the data structure (the fstack proper) where
we will stash and retrieve the numbers coming OR the 875tack.

Chapter 4 - The wits? Famly Scientific FORTH '

e We need to decide how deep the fstack will be, in 80-bit
(TREAL) wide cells, and then ALLOT 10' that number of
bytes of storage.

0 We need a word that will initialize the 80x87 and fstack.

e We need a fast wa to increment (or decrement) the address I
of the next availab e space in the fstack by 10. :
I

e We need to test whether the 87stack is full (or empty). i

0 finally, what do we do if the memory we set aside gets full? !
The solution chosen below makes the extension circular using I
modulo arithmetic to compute the addresses within it.

I
i
We now exhibit the fstack manager program in fig. 4-2 on page 83
below. By now the reader should be familiar enough with FORTH .~
style to understand the program logic with only moderate com-

menting and explanation. Remember— the program reads from
the bottom up!

The key words are FPUSH and FPOP — they do the work. Every
word that changes the 87stack has to be redefined to include
either FPUSH or FPOP as appropriate.

As the test results in Table 4-5 below make clear, the high-level
stack extension is too slow. Some optimization is necessary.

Table 4-5 fstack manager timings”
T8086 machine @ 4.77 MHz

*4o,ooo FPUSH's and FPOP's

 

High-level Optlmlzed Hand-coded

29 sec 6 sec 4 sec

 

 

 

HS/FORI‘H comes with a very helpful utility: a recursive-descent!

optimizer. The optimizer replaces the subroutine calls of ordi-- ‘

nary high-level threaded FORTH code by in-line machine code I

cJuianVNoueiaaa-Alingntsmmed. ,

 

-»

 

WFORTH MQ-TMWFM

 

\noAnmmsermman

beam.

VARIABLE Fs-eiza (shedinckhM‘s)

40 Fselzzi (newsman)

VARIABLE flGTH (la-minimum)

VARIABLEFSP (wmmm

VAniAet£ FDEPTH (annals-duet)
(QWWI

CREATE F3TAG< FS-SIZE @10‘ ALLOT OKLW

:FSINIT FSP OI -8 FDEPTH I FS-SIZE @10'
flGTHI fiNIT ; (IrIUIzeWardstadrs)

000E10+ <96 flCSOAI» swoon:

030E10- <%83EBOA%> END-WE
(Mmtoaddorsumdw)

FSINIT

:WRAPUspntapmodflgri) \rmkeiatnckdrcuar
[flGTH@] UTERAL UNDER + SWAPMOD:

:AWAYI DUP nor l;(adrn-—n) \ueeiuieetoredwoie
:INC-FSP (--iap') FSPDUP@ 10+ WRAP AWAY! ;
:DEC-FSP (--fap') FSP DUP@ 10— WRAP SWAP I;

:INC—FDE’I'H (--Idenh')
FDEPTI-l DUP@ 1+ FS-LENG‘IH @ MIN
AWAY! ;

:DEC-FDEPTH (--idenh')
FDEPTH DUP@ 1- DUP e <
ABORT" FSTAOK UNDERflOW' AWAY!

:FPUSH INC-FDETH O< NOT
IfiNC-FSP FSTACK +

FDECSTP RN! (atlTl--isp)

THEN :

:FPOP DEC-FDEP'IH -1 < NOT
IF FSP@FSTAO( +
Rm fiNCSTP (fep--st[7])
DEC-FSP THEN:

 

 

 

flo.¢.2 HI-Ievellstackmanagerform7

oJflthNaflatm-Alrtgnsnasrved.

84 Chopra“ —nie com Famly SclentmcFORTH

stripping out redundant pushes and pops of the parameter stack.
It is fairly straightforward to construct a similar optimizer for any ‘
FORTH dialect. Alternatively, one can machine code the time-
critical words.

w“

The results of the tests, presented in Table 4-5 on page 82 above,
are interesting:

0 the optimizer does nearly as well as hand-tuned machine code. I

0 An FPUSH or an FPOP takes about 50 ysec on the average,
when coded by hand, @ 4.77 MHz, or about 240 clock cycles.

0 The irreducible minimum on an 8086/80286 —since TREAL
storage from, or retrieval to the 87stack' rs demanded— annot
possibly be less than 170 clock cycles. 1 fetch and 1 store1 .egl'h
main place to save some time would be m moves to or om
memory; the depth of the fstack and the pointer must be kept l
as variables, but FS-SIZE and flGTH do not change and could "
be compiled as literals, thereby saving 30 cycles ln FPUSH and '
10 in FPOP. )

e Substantially greater efficiency (at best another 1. 5x improve-
ment over the code version discussed above) would require a
different algorithm— —based, perhaps, on the 87stack overflow
or underflow interrupt. But because other error conditions can
initiate this interrupt, the testing and decision-making needed
to use this method seemed to me likely to produce equivalent
overhead to the method employed here.

 

\section{Clone wars}

\TallC{S}{everal} companies have produced non-infringing clones of the

Intel 80x87 family of chips. American Micro Devices is a second
source for 80287 and 80387 chips. Cyrix Corporation has
produced 80287 and 80387 equivalents with significantly faster
transcendental functions and moderately faster arithmetic than ,
the Intel originals.

And finally, Integrated Information Technology, Inc. (founded by
the designers of the Weitek chips) has produced the most inter- 1

 

19. Somewhat less, arill?!) clocks, if a 32-bit bus is utilized with the 386/387 pair. See, ¢.g., John H. 1
Crawford and Patrick F. Gelsinger, Programingthe 80386 (Sybex, San Francisco, 1987). L1

COMFORT“

M4-Them7hmly as

eating of the clones: the 80c287/80c387 not only perform arith-
metic significantly faster than the Intel originals, they possess 24
additional 80-bit on-chip registers. Unfortunately these cannot
be combined directly with the eight original registers to make a
32-deep stack, since this would have required increasing the
80x87 stack-pointer (see 57 above) from 3 bits to 5. Since there
was no place to find the 2 extra bits. one cannot really fault IIT's
designers.

But if they cannot be used to extend the 87stack, what are the 24
extra registers good for? Eight (bank 3) are not even accessible,
being used to speed up on-chip arithmetic. However, banlm 0-2
— 24 registers — can be accessed (cg. for on-chip cache memory).
Moreover, IIT has provided an on-chip linear transformation: a
4x4 matrix multiplies a 4-dimensional column vector in place. (The
original vector is overwritten, but the matrix is unchanged.) It worls
like this :

first we define the instructions

CODE FSBPO <96 DB E8 96> END-CODE
CODE FSBP1 <96 DB EB 96> END-CODE
CODE FSBP2 <96 DB EA 96> END-CODE
CODE F4x4 <96 DB F1 96 > END-CODE

Then we load the 4x4 matrix into banks 1 and 2, the vector into
bank 0, and multiply:

o VAR a{{ o VAR v{

: VEC- > 087 ( adr ~ - ) \ load vector into 80c387
IS V{ \ I vector address to V{
fiNIT FSBPO \ reset ST. select bank 0

30 DOV{I} G@ LOOP ;
: MAT->087 (adr--) \Ioad matrix into 80c387

IS a{{ \ ! matrix address to a{{
fiNIT FSBP2 \ reset ST, select bank 2
1 0 DO

30 DO a{{IJ}} G@ LOOP
P \cont'd

 

20. Note: fiNIT operates difierently on IIT’s coprocessors than on Intel’s. It does not place NAN‘s
in all 8 87stack cells.

30 DO v{l} GI LOOP;
\example: ANS = A*V
A{{ MAT->87 V{ VEC->87 F4x4 ANs{ce7->VEC

Chapter 4 — rite eoxa-r FamIIy Scientific FORTH 5‘
fiNIT FSBP1 \ reset ST, select bank 1
3 2 DO
SODOa{{IJ}}G@ LOOP
LOOP ; I
I
087- >VEC (adr - - ) \ I from 800387 to vector 1
IS V{ \ I vector address to v{ ‘
FSBPO \ select bank 0 %

 

\TallC{T}{he} on-chip 4x4 matrix multiply was intended to accelerate

3-dimensional graphics (rotation and translation can be ex-
pressed as a single 4-dimensional transformation). However, most
scientific programmers spend little time on 3-D graphics. The matrix
instructions are more interesting for their potential to accelerate
general matrix operations21. For example, suppose we need to -_
transform a vector by multiplying with an arbitrary matrix. Normally i
we would write ‘

   

II
Yi = 21 Aijxj (7) i
J:

But consider, e.g., an 8 x 8 matrix operating on an 8-dimensional i
column vector: we partition the matrix and vector into 4-dimen- g
sional sub-matrices an , etc. and sub-vectors x1 , etc.: '

_ an 012 X1 _ 01131 + 611212
[A] [x] _ [021 022] [‘2] [anti + 022352] (8)
From timings on assembly-coded demonstration programs that
multiply vectors by constant 4 x 4 matrices, we estimate an overall
speedup of 6—7 -fold on an 80c387 system. The timings for this
process are as shown in Table 4-6 below (assume 32-bit REAL'4
matrices). The execution time for a 4-dimensional linear trans-
formation using conventional operations is approximately 1744

clock cycles. The time using the vector operation is some 30-300
clocks, based on measured performance. Since 256 clocks are

 

21.

See Ch. 9 of this book for a fuller discussion of standard matrix algorithms.

MPORTH

awa-mmrrm O7

needed to load and store a 4-dimensional vector, we therefore
estimate the vector operation F4“ takes only about 50 clocks,
ale. about I floating point multiplication time. We can now es-
timate the time to multiply two 4 x 4 matrices as about 1200 cloclu
vs. about 7000 for the scalar process, Le. the same speedup factor

Table 46 Timings for 4x4 matrix - vector

 

Operation: x + @ I
mx mm: 16-52 12'28 20-20 4'44

W: 832 336 400 176

 

 

 

as for matrix 'vector. If one must load the 4x4 matrix each time,
the speedup factor is less: about 35-fold because of the 16 addi-
tional fetches.

The conventionally programmed 8x8 matrix-vector multiply
should also be some 3-5 times faster than the scalar operations,
Le. there is no obvious speed gain — except being able to employ
the built-in vector instruction F4x4 — from partitioning the 8x 8
system into 4X4 sub-units. However, Strassen22 has pointed out
that if one can evaluate matrix products recursively, partitioning
can substantially speed the most time—consuming matrix opera-
tions, multiplication and inversion. For example, it appears as
though the product of two partitioned matrices,

[A] [B] = [::::;:][::::Z::] [c]

[C] = [allbll+alzb21 allblz+alzb22]

azlbll+azzb21 azlblz+azzbzz

(9)

 

22. v. Strassen, Numer. Math. 13 (1969) 134. See also v. Put, SIAMReview 26 (1984) 393.

cJtsanVNouetaaz-Aiingmm.

Chaptar4-Thewxa7Famly Scleno‘flcFORTH

requires 8 matrix multiplications and 4 matrix additions to
evaluate. Strassen has shown that in fact the evaluation can be
performed with 7 matrix multiplications:

p1 = (all + an) (b11 + bu)

p2 = (an + an) bn

p3 = all (b12 — b2)

p, = (—a,, + an) (b,1 + bu) (10)
p5 = (all + an) b22

p5 = an (-bll + bu)

P 7 = (012 ‘ (122) (1721 + bzz)

and 18 matrix additions:

Cu =P1‘P5 +176 +P7
€12=P3+Ps
(11)
€21 =P2 +P6
€22 =P1—P2 +P3 +P4

‘ V‘s-‘-

‘l firm V' was...“

Equations 10 and 11 look at first blush half as efficient as 8 .

multiplications and 4 additions. But let us examine the time to
multiply two partitioned matrices, first by the straightforward
method and then by Strassen's: clearly,

where Mn is the multiplication time andAn the addition time, for
square matrices of order n.

Setting n = 2" that (note A,l a! 0(n2) ) we see the recursion,
Eq. 12, is satisfied by an expression of form

tun-ml.“ +ca 4“ (13)

M4-Tham7Fan-ily I.

where m and I are the elementary multiplication and addition
times. Substituting 13in 12 we find A =- 8 and c - — 1, i.e.,

Mn - mn’ — an2 (14)
Applying the same idea to Strassen’s method we obtain

10,, = 710,, + um2 (15)
or

Ian-mn’w—San’, (16)
where

lg? E I092? = 2.807...

That is, partitioning allows a potentially large reduction in the
time to multiply dense matrices.
\TallC{B}{y} writing a partitioned matrix in the form
an at: | 0 all 812 ..
= = 1
[A] [82‘ 82;] [52131—11 ' i 0 z i ( I)

where

 

z = 322 — anal—9a.. (18)

I 0
[—azrar-t1 '] (19)

which leads to the recursion for In , the time to invert an nxn
matrix:

we may express the inverse of A as

[Al-ail?"

 

whose solution is

1,,=mn’97+0(n) (21)

OWVNouatm-Aldd'itsraaarvad.

Chapter 4 - The 80x87 Family Scientific FORTH

ie., the time needed to invert is comparable with that needed to i
multiply.

\TallC{S}{uppose} we merely wish to solve a linear system without invert-
1ng the matrix. can we gain some speed that way? From 17 we
see that the problem

 

Ax=y

reduces to three sub-problems: i

aiiui = Vt (223)

l
3‘2 = Y2 - aztut (22b) ;
a1‘l-x1 =.V1 — ataxz i (22C) i

that is, we have the recursion

 

32,: as, +mn’97+2mn2 (B)
whose solution is dominated by

1
Sn=m(j4-n’g7

+ 2n 2) + O(n ’93) (24)

Linear equation solution via recursion thus has the same asymptotic running time as matrix multiplication, except that 4 x fewer operations are required than for inversion. That is, it should be about 5 x faster to solve a dense system of 1000 linear equations by recursive partitioning than by ordinary Gaussian elimination even on a scalar processor. The vector instruction on the IT 80c387 chip, together with Strassen’ s algorithm, offers the possibility of solving very large systems in practical times, on desktop computers.